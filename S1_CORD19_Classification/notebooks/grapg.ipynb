{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259ee935",
   "metadata": {},
   "source": [
    "# Construction Professionnelle de Graphe d'Articles Scientifiques\n",
    "\n",
    "**Objectif**: Construire un graphe d'articles optimis√© pour GNN et analyse de communaut√©s\n",
    "\n",
    "## Pipeline:\n",
    "1. Chargement des embeddings et m√©tadonn√©es\n",
    "2. Construction du graphe k-NN avec poids de similarit√©\n",
    "3. D√©tection des communaut√©s (Louvain)\n",
    "4. Analyse topologique avanc√©e\n",
    "5. Export pour GNN (PyTorch Geometric format)\n",
    "6. Visualisations professionnelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5f2ae",
   "metadata": {},
   "source": [
    "## üì¶ Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f3d698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports r√©ussis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from community import community_louvain\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97959da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ R√©pertoire de sortie: ..\\data\\processed\\graph_outputs\n",
      "üîß K-voisins: 10\n",
      "üîß Seuil de similarit√©: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration du projet\n",
    "BASE_PATH = Path(\"../data/processed\")\n",
    "OUTPUT_PATH = BASE_PATH / \"graph_outputs\"\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Fichiers d'entr√©e\n",
    "EMBEDDINGS_PATH = BASE_PATH / \"embeddings.npy\"\n",
    "ARTICLES_PATH = BASE_PATH / \"cleaned_articles.csv\"\n",
    "\n",
    "# Param√®tres du graphe\n",
    "K_NEIGHBORS = 10\n",
    "SIMILARITY_THRESHOLD = 0.2\n",
    "USE_COSINE = True\n",
    "SAMPLE_SIZE = None  # None = tout le dataset\n",
    "\n",
    "print(f\"üìÇ R√©pertoire de sortie: {OUTPUT_PATH}\")\n",
    "print(f\"üîß K-voisins: {K_NEIGHBORS}\")\n",
    "print(f\"üîß Seuil de similarit√©: {SIMILARITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ca5f8",
   "metadata": {},
   "source": [
    "## üìÇ 1. Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7fdec21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìÇ CHARGEMENT DES DONN√âES\n",
      "======================================================================\n",
      "\n",
      "üîÑ Chargement des embeddings...\n",
      "   ‚úÖ Shape: (816359, 384)\n",
      "   ‚úÖ Type: float32\n",
      "   ‚úÖ Taille m√©moire: 1195.84 MB\n",
      "\n",
      "üîÑ Chargement des articles...\n",
      "   ‚úÖ 816,359 articles charg√©s\n",
      "   ‚úÖ Colonnes: ['cord_uid', 'sha', 'source_x', 'title', 'doi']...\n",
      "\n",
      "‚úÖ Donn√©es finales: 816,359 articles, dim=384\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÇ CHARGEMENT DES DONN√âES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger embeddings\n",
    "print(f\"\\nüîÑ Chargement des embeddings...\")\n",
    "if not EMBEDDINGS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Embeddings introuvables: {EMBEDDINGS_PATH}\")\n",
    "\n",
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "print(f\"   ‚úÖ Shape: {embeddings.shape}\")\n",
    "print(f\"   ‚úÖ Type: {embeddings.dtype}\")\n",
    "print(f\"   ‚úÖ Taille m√©moire: {embeddings.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Charger articles\n",
    "print(f\"\\nüîÑ Chargement des articles...\")\n",
    "if not ARTICLES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Articles introuvables: {ARTICLES_PATH}\")\n",
    "\n",
    "articles_df = pd.read_csv(ARTICLES_PATH)\n",
    "print(f\"   ‚úÖ {len(articles_df):,} articles charg√©s\")\n",
    "print(f\"   ‚úÖ Colonnes: {list(articles_df.columns[:5])}...\")\n",
    "\n",
    "# Synchroniser tailles\n",
    "n_samples = min(len(articles_df), embeddings.shape[0])\n",
    "articles_df = articles_df.iloc[:n_samples].reset_index(drop=True)\n",
    "embeddings = embeddings[:n_samples]\n",
    "\n",
    "# √âchantillonnage optionnel\n",
    "if SAMPLE_SIZE and n_samples > SAMPLE_SIZE:\n",
    "    print(f\"\\n‚ö†Ô∏è  √âchantillonnage: {SAMPLE_SIZE} articles\")\n",
    "    indices = np.random.RandomState(42).choice(n_samples, SAMPLE_SIZE, replace=False)\n",
    "    articles_df = articles_df.iloc[indices].reset_index(drop=True)\n",
    "    embeddings = embeddings[indices]\n",
    "\n",
    "print(f\"\\n‚úÖ Donn√©es finales: {len(articles_df):,} articles, dim={embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa041c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Aper√ßu des articles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>OBJECTIVE: This retrospective chart review des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>Inflammatory diseases of the respiratory tract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>Surfactant protein-D (SP-D) participates in th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Clinical features of culture-proven Mycoplasma...   \n",
       "1  Nitric oxide: a pro-inflammatory mediator in l...   \n",
       "2    Surfactant protein-D and pulmonary host defense   \n",
       "\n",
       "                                            abstract  \n",
       "0  OBJECTIVE: This retrospective chart review des...  \n",
       "1  Inflammatory diseases of the respiratory tract...  \n",
       "2  Surfactant protein-D (SP-D) participates in th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Statistiques des embeddings:\n",
      "   Moyenne: -0.0008\n",
      "   Std: 0.0510\n",
      "   Min: -0.2988\n",
      "   Max: 0.2857\n"
     ]
    }
   ],
   "source": [
    "# Aper√ßu des donn√©es\n",
    "print(\"\\nüìä Aper√ßu des articles:\")\n",
    "display(articles_df[['title', 'abstract']].head(3))\n",
    "\n",
    "print(\"\\nüìä Statistiques des embeddings:\")\n",
    "print(f\"   Moyenne: {embeddings.mean():.4f}\")\n",
    "print(f\"   Std: {embeddings.std():.4f}\")\n",
    "print(f\"   Min: {embeddings.min():.4f}\")\n",
    "print(f\"   Max: {embeddings.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c4d2e",
   "metadata": {},
   "source": [
    "## üîó 2. Construction du Graphe k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef618b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîó CONSTRUCTION DU GRAPHE k-NN OPTIMIS√âE (sklearn)\n",
      "======================================================================\n",
      "\n",
      "üîß Configuration:\n",
      "   Articles: 100,000\n",
      "   K-voisins: 10\n",
      "   Seuil: 0.35\n",
      "   Batch size: 20,000\n",
      "   CPU cores: tous disponibles\n",
      "\n",
      "üîß Normalisation des embeddings...\n",
      "   ‚úÖ Normes moyennes: 1.0000\n",
      "\n",
      "üöÄ MODE BATCH (dataset large: 100,000 articles)\n",
      "   Construction de l'index k-NN une seule fois...\n",
      "   ‚úÖ Index k-NN construit\n",
      "\n",
      "üîç Recherche k-NN par batch de 20,000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122c0203abc4e96846aaa02fe23a79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "k-NN batch search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     86\u001b[39m batch = embeddings_normalized[i:end_idx]\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Recherche pour ce batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m distances, indices = \u001b[43mnn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m all_distances.append(distances)\n\u001b[32m     92\u001b[39m all_indices.append(indices)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:906\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m         kwds = \u001b[38;5;28mself\u001b[39m.effective_metric_params_\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     chunked_results = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    907\u001b[39m         pairwise_distances_chunked(\n\u001b[32m    908\u001b[39m             X,\n\u001b[32m    909\u001b[39m             \u001b[38;5;28mself\u001b[39m._fit_X,\n\u001b[32m    910\u001b[39m             reduce_func=reduce_func,\n\u001b[32m    911\u001b[39m             metric=\u001b[38;5;28mself\u001b[39m.effective_metric_,\n\u001b[32m    912\u001b[39m             n_jobs=n_jobs,\n\u001b[32m    913\u001b[39m             **kwds,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    915\u001b[39m     )\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mball_tree\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkd_tree\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2249\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2248\u001b[39m     chunk_size = D_chunk.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2249\u001b[39m     D_chunk = \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:750\u001b[39m, in \u001b[36mKNeighborsMixin._kneighbors_reduce_func\u001b[39m\u001b[34m(self, dist, start, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    723\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[32m    724\u001b[39m \n\u001b[32m    725\u001b[39m \u001b[33;03mCallback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    747\u001b[39m \u001b[33;03m    The neighbors indices.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    749\u001b[39m sample_range = np.arange(dist.shape[\u001b[32m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m neigh_ind = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m neigh_ind = neigh_ind[:, :n_neighbors]\n\u001b[32m    752\u001b[39m \u001b[38;5;66;03m# argpartition doesn't guarantee sorted order, so we sort again\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:961\u001b[39m, in \u001b[36margpartition\u001b[39m\u001b[34m(a, kth, axis, kind, order)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margpartition\u001b[39m(a, kth, axis=-\u001b[32m1\u001b[39m, kind=\u001b[33m'\u001b[39m\u001b[33mintroselect\u001b[39m\u001b[33m'\u001b[39m, order=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    877\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m \n\u001b[32m    960\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margpartition\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE OPTIMIS√â √Ä REMPLACER DANS LE NOTEBOOK (SANS FAISS)\n",
    "# Section: Construction du Graphe k-NN\n",
    "# Optimisations: batch processing, sparse graph, memory efficient\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "import gc\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîó CONSTRUCTION DU GRAPHE k-NN OPTIMIS√âE (sklearn)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ========================\n",
    "# PARAM√àTRES OPTIMIS√âS\n",
    "# ========================\n",
    "K_NEIGHBORS = 10\n",
    "SIMILARITY_THRESHOLD = 0.35  # Plus √©lev√© = graphe plus sparse = plus rapide\n",
    "BATCH_SIZE = 20000  # Batch size pour la recherche\n",
    "N_JOBS = -1  # Utiliser tous les CPU disponibles\n",
    "\n",
    "# Option 1: √âchantillonnage pour test rapide (recommand√© pour 800K articles)\n",
    "# SAMPLE_SIZE = 100000  # D√©commenter pour tester sur 100K articles d'abord\n",
    "SAMPLE_SIZE = None  # None = tous les articles (attention: tr√®s lent!)\n",
    "\n",
    "print(f\"\\nüîß Configuration:\")\n",
    "print(f\"   Articles: {len(articles_df):,}\")\n",
    "print(f\"   K-voisins: {K_NEIGHBORS}\")\n",
    "print(f\"   Seuil: {SIMILARITY_THRESHOLD}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"   CPU cores: tous disponibles\")\n",
    "if SAMPLE_SIZE:\n",
    "    print(f\"   ‚ö†Ô∏è  MODE √âCHANTILLON: {SAMPLE_SIZE:,} articles\")\n",
    "\n",
    "# ========================\n",
    "# √âCHANTILLONNAGE (optionnel mais recommand√©)\n",
    "# ========================\n",
    "if SAMPLE_SIZE and len(embeddings) > SAMPLE_SIZE:\n",
    "    print(f\"\\nüìä √âchantillonnage de {SAMPLE_SIZE:,} articles...\")\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(embeddings), SAMPLE_SIZE, replace=False)\n",
    "    sample_indices = np.sort(sample_indices)  # Garder l'ordre\n",
    "    \n",
    "    embeddings = embeddings[sample_indices]\n",
    "    articles_df = articles_df.iloc[sample_indices].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ √âchantillon cr√©√©: {len(embeddings):,} articles\")\n",
    "\n",
    "n_samples = len(embeddings)\n",
    "\n",
    "# ========================\n",
    "# NORMALISATION\n",
    "# ========================\n",
    "print(f\"\\nüîß Normalisation des embeddings...\")\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "norms[norms == 0] = 1.0\n",
    "embeddings_normalized = embeddings / norms\n",
    "print(f\"   ‚úÖ Normes moyennes: {np.linalg.norm(embeddings_normalized, axis=1).mean():.4f}\")\n",
    "\n",
    "# ========================\n",
    "# OPTION A: Construction par batch (RECOMMAND√â pour 800K articles)\n",
    "# ========================\n",
    "if n_samples > 50000:\n",
    "    print(f\"\\nüöÄ MODE BATCH (dataset large: {n_samples:,} articles)\")\n",
    "    print(f\"   Construction de l'index k-NN une seule fois...\")\n",
    "    \n",
    "    # Construire l'index une fois\n",
    "    nn_model = NearestNeighbors(\n",
    "        n_neighbors=K_NEIGHBORS + 1,\n",
    "        metric='cosine',\n",
    "        algorithm='brute',  # Plus rapide pour cosine sur datasets moyens\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "    nn_model.fit(embeddings_normalized)\n",
    "    print(f\"   ‚úÖ Index k-NN construit\")\n",
    "    \n",
    "    # Recherche par batch pour √©conomiser m√©moire\n",
    "    print(f\"\\nüîç Recherche k-NN par batch de {BATCH_SIZE:,}...\")\n",
    "    \n",
    "    all_distances = []\n",
    "    all_indices = []\n",
    "    \n",
    "    for i in tqdm(range(0, n_samples, BATCH_SIZE), desc=\"k-NN batch search\"):\n",
    "        end_idx = min(i + BATCH_SIZE, n_samples)\n",
    "        batch = embeddings_normalized[i:end_idx]\n",
    "        \n",
    "        # Recherche pour ce batch\n",
    "        distances, indices = nn_model.kneighbors(batch, return_distance=True)\n",
    "        \n",
    "        all_distances.append(distances)\n",
    "        all_indices.append(indices)\n",
    "        \n",
    "        # Lib√©rer m√©moire toutes les 5 batchs\n",
    "        if (i // BATCH_SIZE) % 5 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    # Concat√©ner tous les r√©sultats\n",
    "    distances = np.vstack(all_distances)\n",
    "    indices = np.vstack(all_indices)\n",
    "    \n",
    "    # Convertir en similarit√©s\n",
    "    similarities = 1.0 - distances\n",
    "    \n",
    "    # Lib√©rer m√©moire\n",
    "    del all_distances, all_indices, nn_model\n",
    "    gc.collect()\n",
    "\n",
    "# ========================\n",
    "# OPTION B: Construction directe (pour datasets < 50K)\n",
    "# ========================\n",
    "else:\n",
    "    print(f\"\\nüöÄ MODE DIRECT (dataset petit: {n_samples:,} articles)\")\n",
    "    \n",
    "    nn_model = NearestNeighbors(\n",
    "        n_neighbors=K_NEIGHBORS + 1,\n",
    "        metric='cosine',\n",
    "        algorithm='auto',\n",
    "        n_jobs=N_JOBS\n",
    "    )\n",
    "    nn_model.fit(embeddings_normalized)\n",
    "    \n",
    "    print(f\"   Recherche k-NN...\")\n",
    "    distances, indices = nn_model.kneighbors(embeddings_normalized, return_distance=True)\n",
    "    similarities = 1.0 - distances\n",
    "    \n",
    "    del nn_model\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ k-NN termin√©\")\n",
    "print(f\"   Similarit√© moyenne: {similarities[:, 1:].mean():.4f}\")\n",
    "print(f\"   Similarit√© min: {similarities[:, 1:].min():.4f}\")\n",
    "print(f\"   Similarit√© max: {similarities[:, 1:].max():.4f}\")\n",
    "\n",
    "# Lib√©rer m√©moire\n",
    "del embeddings_normalized\n",
    "gc.collect()\n",
    "\n",
    "# ========================\n",
    "# CONSTRUCTION DES AR√äTES (OPTIMIS√âE avec matrice sparse)\n",
    "# ========================\n",
    "print(f\"\\nüèóÔ∏è  Construction des ar√™tes (seuil={SIMILARITY_THRESHOLD})...\")\n",
    "print(f\"   Utilisation de matrice sparse pour efficacit√© m√©moire...\")\n",
    "\n",
    "# Utiliser une matrice sparse pour construction plus rapide\n",
    "edge_matrix = lil_matrix((n_samples, n_samples), dtype=np.float32)\n",
    "edge_count = 0\n",
    "\n",
    "for i in tqdm(range(indices.shape[0]), desc=\"Building edges\", mininterval=1.0):\n",
    "    for j in range(1, indices.shape[1]):  # Skip self (j=0)\n",
    "        neighbor_idx = int(indices[i, j])\n",
    "        similarity = float(similarities[i, j])\n",
    "        \n",
    "        # Filtrer par seuil\n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            # √âviter doublons (stocker seulement i < j)\n",
    "            if i < neighbor_idx:\n",
    "                edge_matrix[i, neighbor_idx] = similarity\n",
    "                edge_count += 1\n",
    "            elif i > neighbor_idx:\n",
    "                edge_matrix[neighbor_idx, i] = similarity\n",
    "                edge_count += 1\n",
    "    \n",
    "    # Affichage p√©riodique\n",
    "    if (i + 1) % 50000 == 0:\n",
    "        print(f\"      {edge_count:,} ar√™tes cr√©√©es jusqu'√† pr√©sent...\")\n",
    "        gc.collect()\n",
    "\n",
    "# Convertir en format CSR (plus efficace pour NetworkX)\n",
    "edge_matrix = edge_matrix.tocsr()\n",
    "\n",
    "print(f\"\\n‚úÖ {edge_count:,} ar√™tes uniques cr√©√©es\")\n",
    "print(f\"   Ar√™tes par n≈ìud (moyenne): {edge_count / n_samples:.2f}\")\n",
    "\n",
    "# V√©rifier si le graphe n'est pas trop sparse\n",
    "if edge_count / n_samples < 2:\n",
    "    print(f\"   ‚ö†Ô∏è  Graphe tr√®s sparse! R√©duisez SIMILARITY_THRESHOLD √† 0.25-0.3\")\n",
    "elif edge_count / n_samples > 50:\n",
    "    print(f\"   ‚ö†Ô∏è  Graphe tr√®s dense! Augmentez SIMILARITY_THRESHOLD √† 0.4-0.5\")\n",
    "\n",
    "# Lib√©rer m√©moire\n",
    "del distances, indices, similarities\n",
    "gc.collect()\n",
    "\n",
    "# ========================\n",
    "# CR√âATION DU GRAPHE NETWORKX (OPTIMIS√âE)\n",
    "# ========================\n",
    "print(f\"\\nüî® Cr√©ation du graphe NetworkX...\")\n",
    "\n",
    "# Cr√©er graphe vide\n",
    "G = nx.Graph()\n",
    "\n",
    "# Ajouter n≈ìuds avec attributs (optimis√©)\n",
    "print(f\"   Ajout de {n_samples:,} n≈ìuds...\")\n",
    "node_data = [\n",
    "    (i, {\n",
    "        'title': str(row.get('title', ''))[:150],\n",
    "        'source': str(row.get('source_x', ''))[:50]\n",
    "    })\n",
    "    for i, row in articles_df.iterrows()\n",
    "]\n",
    "G.add_nodes_from(node_data)\n",
    "del node_data\n",
    "gc.collect()\n",
    "\n",
    "# Ajouter ar√™tes depuis la matrice sparse\n",
    "print(f\"   Ajout de {edge_count:,} ar√™tes...\")\n",
    "edges_to_add = []\n",
    "\n",
    "# Extraire les ar√™tes de la matrice sparse\n",
    "rows, cols = edge_matrix.nonzero()\n",
    "for idx in tqdm(range(len(rows)), desc=\"Extracting edges\", mininterval=1.0):\n",
    "    i, j = rows[idx], cols[idx]\n",
    "    weight = edge_matrix[i, j]\n",
    "    edges_to_add.append((int(i), int(j), float(weight)))\n",
    "    \n",
    "    # Ajouter par batch de 50K pour √©viter OOM\n",
    "    if len(edges_to_add) >= 50000:\n",
    "        G.add_weighted_edges_from(edges_to_add)\n",
    "        edges_to_add = []\n",
    "        gc.collect()\n",
    "\n",
    "# Ajouter les ar√™tes restantes\n",
    "if edges_to_add:\n",
    "    G.add_weighted_edges_from(edges_to_add)\n",
    "\n",
    "del edge_matrix, edges_to_add\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Graphe cr√©√©:\")\n",
    "print(f\"   N≈ìuds: {G.number_of_nodes():,}\")\n",
    "print(f\"   Ar√™tes: {G.number_of_edges():,}\")\n",
    "print(f\"   Densit√©: {nx.density(G):.8f}\")\n",
    "\n",
    "# ========================\n",
    "# STATISTIQUES RAPIDES\n",
    "# ========================\n",
    "print(f\"\\nüìä Statistiques rapides:\")\n",
    "degrees = dict(G.degree())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "print(f\"   Degr√© moyen: {np.mean(degree_values):.2f}\")\n",
    "print(f\"   Degr√© m√©dian: {np.median(degree_values):.0f}\")\n",
    "print(f\"   Degr√© min: {min(degree_values)}\")\n",
    "print(f\"   Degr√© max: {max(degree_values)}\")\n",
    "\n",
    "# N≈ìuds isol√©s\n",
    "isolated = [n for n in G.nodes() if G.degree(n) == 0]\n",
    "if isolated:\n",
    "    print(f\"   ‚ö†Ô∏è  N≈ìuds isol√©s: {len(isolated)} ({len(isolated)/n_samples*100:.2f}%)\")\n",
    "    print(f\"       Consid√©rez r√©duire SIMILARITY_THRESHOLD\")\n",
    "\n",
    "print(f\"\\nüíæ M√©moire lib√©r√©e, pr√™t pour la d√©tection de communaut√©s...\")\n",
    "\n",
    "# ========================\n",
    "# CONSEIL POUR ACC√âL√âRER ENCORE PLUS\n",
    "# ========================\n",
    "print(f\"\\nüí° CONSEIL PERFORMANCE:\")\n",
    "print(f\"   Pour 800K+ articles, consid√©rez:\")\n",
    "print(f\"   1. √âchantillonner 100K-200K articles (d√©commenter SAMPLE_SIZE)\")\n",
    "print(f\"   2. Augmenter SIMILARITY_THRESHOLD √† 0.4-0.5\")\n",
    "print(f\"   3. R√©duire K_NEIGHBORS √† 5-8\")\n",
    "print(f\"   4. Ou installer FAISS: pip install faiss-cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cfd8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîó CONSTRUCTION DU GRAPHE k-NN\n",
      "======================================================================\n",
      "\n",
      "üîß Normalisation des embeddings (cosine similarity)...\n",
      "   ‚úÖ Normes moyennes: 1.0000\n",
      "\n",
      "üîÑ Construction de l'index k-NN (k=10)...\n",
      "   ‚úÖ Index k-NN construit (metric=cosine)\n",
      "\n",
      "üîç Recherche des 10 plus proches voisins...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Recherche des voisins\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîç Recherche des \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mK_NEIGHBORS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m plus proches voisins...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m distances, indices = \u001b[43mnn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Calculer les poids (similarit√©s)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_COSINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:906\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m         kwds = \u001b[38;5;28mself\u001b[39m.effective_metric_params_\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     chunked_results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mball_tree\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkd_tree\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2240\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2239\u001b[39m     X_chunk = X[sl]\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m D_chunk = \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[32m   2242\u001b[39m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2243\u001b[39m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[32m   2244\u001b[39m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[32m   2245\u001b[39m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m     D_chunk.flat[sl.start :: _num_samples(X) + \u001b[32m1\u001b[39m] = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2476\u001b[39m, in \u001b[36mpairwise_distances\u001b[39m\u001b[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[39m\n\u001b[32m   2473\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001b[32m   2474\u001b[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001b[32m-> \u001b[39m\u001b[32m2476\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1965\u001b[39m, in \u001b[36m_parallel_pairwise\u001b[39m\u001b[34m(X, Y, func, n_jobs, **kwds)\u001b[39m\n\u001b[32m   1963\u001b[39m fd = delayed(_dist_wrapper)\n\u001b[32m   1964\u001b[39m ret = np.empty((X.shape[\u001b[32m0\u001b[39m], Y.shape[\u001b[32m0\u001b[39m]), dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreading\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_n_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[32m   1971\u001b[39m     \u001b[38;5;66;03m# zeroing diagonal for euclidean norm.\u001b[39;00m\n\u001b[32m   1972\u001b[39m     \u001b[38;5;66;03m# TODO: do it also for other norms.\u001b[39;00m\n\u001b[32m   1973\u001b[39m     np.fill_diagonal(ret, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîó CONSTRUCTION DU GRAPHE k-NN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Normaliser les embeddings pour similarit√© cosinus\n",
    "if USE_COSINE:\n",
    "    print(\"\\nüîß Normalisation des embeddings (cosine similarity)...\")\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    embeddings_normalized = embeddings / norms\n",
    "    print(f\"   ‚úÖ Normes moyennes: {np.linalg.norm(embeddings_normalized, axis=1).mean():.4f}\")\n",
    "else:\n",
    "    embeddings_normalized = embeddings\n",
    "\n",
    "# Construire l'index k-NN\n",
    "print(f\"\\nüîÑ Construction de l'index k-NN (k={K_NEIGHBORS})...\")\n",
    "metric = \"cosine\" if USE_COSINE else \"euclidean\"\n",
    "nn_model = NearestNeighbors(n_neighbors=K_NEIGHBORS + 1, metric=metric, n_jobs=-1)\n",
    "nn_model.fit(embeddings_normalized)\n",
    "print(f\"   ‚úÖ Index k-NN construit (metric={metric})\")\n",
    "\n",
    "# Recherche des voisins\n",
    "print(f\"\\nüîç Recherche des {K_NEIGHBORS} plus proches voisins...\")\n",
    "distances, indices = nn_model.kneighbors(embeddings_normalized, return_distance=True)\n",
    "\n",
    "# Calculer les poids (similarit√©s)\n",
    "if USE_COSINE:\n",
    "    weights = 1.0 - distances  # Cosine similarity\n",
    "else:\n",
    "    weights = 1.0 / (1.0 + distances)  # Inverse distance\n",
    "\n",
    "print(f\"   ‚úÖ Voisins trouv√©s\")\n",
    "print(f\"   ‚úÖ Similarit√© moyenne: {weights[:, 1:].mean():.4f}\")\n",
    "print(f\"   ‚úÖ Similarit√© min: {weights[:, 1:].min():.4f}\")\n",
    "print(f\"   ‚úÖ Similarit√© max: {weights[:, 1:].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire la liste des ar√™tes avec filtrage\n",
    "print(f\"\\nüèóÔ∏è  Construction des ar√™tes (seuil={SIMILARITY_THRESHOLD})...\")\n",
    "\n",
    "edges = []\n",
    "for i in tqdm(range(indices.shape[0]), desc=\"Building edges\"):\n",
    "    for j in range(1, indices.shape[1]):  # Skip self (j=0)\n",
    "        neighbor_idx = int(indices[i, j])\n",
    "        similarity = float(weights[i, j])\n",
    "        \n",
    "        # Filtrer par seuil de similarit√©\n",
    "        if similarity >= SIMILARITY_THRESHOLD:\n",
    "            edges.append((i, neighbor_idx, similarity))\n",
    "\n",
    "print(f\"\\n‚úÖ {len(edges):,} ar√™tes cr√©√©es\")\n",
    "print(f\"   Ar√™tes par n≈ìud (moyenne): {len(edges) / len(articles_df):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le graphe NetworkX\n",
    "print(\"\\nüî® Cr√©ation du graphe NetworkX...\")\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "# Ajouter les n≈ìuds avec attributs\n",
    "print(\"   Ajout des n≈ìuds...\")\n",
    "for i, row in articles_df.iterrows():\n",
    "    G.add_node(i, \n",
    "               title=str(row.get('title', '')),\n",
    "               abstract=str(row.get('abstract', ''))[:500],  # Limiter taille\n",
    "               source=str(row.get('source_x', '')),\n",
    "               year=int(row.get('year', 0)) if pd.notna(row.get('year')) else 0)\n",
    "\n",
    "# Ajouter les ar√™tes avec poids\n",
    "print(\"   Ajout des ar√™tes...\")\n",
    "for i, j, weight in tqdm(edges, desc=\"Adding edges\"):\n",
    "    if i != j:  # √âviter self-loops\n",
    "        if G.has_edge(i, j):\n",
    "            # Garder le poids maximum si ar√™te existe d√©j√†\n",
    "            if weight > G[i][j]['weight']:\n",
    "                G[i][j]['weight'] = weight\n",
    "        else:\n",
    "            G.add_edge(i, j, weight=weight)\n",
    "\n",
    "print(f\"\\n‚úÖ Graphe cr√©√©:\")\n",
    "print(f\"   N≈ìuds: {G.number_of_nodes():,}\")\n",
    "print(f\"   Ar√™tes: {G.number_of_edges():,}\")\n",
    "print(f\"   Densit√©: {nx.density(G):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c12d4b",
   "metadata": {},
   "source": [
    "## üß© 3. D√©tection des Communaut√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e88455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß© D√âTECTION DES COMMUNAUT√âS (Louvain)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîÑ Ex√©cution de l'algorithme de Louvain...\")\n",
    "partition = community_louvain.best_partition(G, weight='weight', random_state=42)\n",
    "\n",
    "# Ajouter les communaut√©s au dataframe\n",
    "communities = pd.Series(partition).sort_index().values\n",
    "articles_df['community'] = communities\n",
    "\n",
    "# Statistiques\n",
    "n_communities = int(articles_df['community'].nunique())\n",
    "community_sizes = articles_df['community'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ {n_communities} communaut√©s d√©tect√©es\")\n",
    "print(f\"\\nüìä Distribution des tailles:\")\n",
    "print(f\"   Taille moyenne: {community_sizes.mean():.1f} articles\")\n",
    "print(f\"   M√©diane: {community_sizes.median():.0f} articles\")\n",
    "print(f\"   Plus grande: {community_sizes.max():,} articles\")\n",
    "print(f\"   Plus petite: {community_sizes.min()} articles\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 10 des communaut√©s:\")\n",
    "for comm_id, size in community_sizes.head(10).items():\n",
    "    print(f\"   Community {comm_id}: {size:,} articles ({size/len(articles_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd208d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la modularit√©\n",
    "modularity = community_louvain.modularity(partition, G, weight='weight')\n",
    "print(f\"\\nüìà Modularit√©: {modularity:.4f}\")\n",
    "if modularity > 0.4:\n",
    "    print(\"   ‚úÖ Excellente structure communautaire\")\n",
    "elif modularity > 0.3:\n",
    "    print(\"   ‚úÖ Bonne structure communautaire\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Structure communautaire faible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f16b8",
   "metadata": {},
   "source": [
    "## üìä 4. Analyse Topologique Avanc√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac34155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ANALYSE TOPOLOGIQUE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# M√©triques de base\n",
    "print(\"\\nüî¢ M√©triques de base:\")\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "density = nx.density(G)\n",
    "\n",
    "print(f\"   N≈ìuds: {n_nodes:,}\")\n",
    "print(f\"   Ar√™tes: {n_edges:,}\")\n",
    "print(f\"   Densit√©: {density:.6f}\")\n",
    "print(f\"   Type: {'Connexe' if nx.is_connected(G) else 'D√©connect√©'}\")\n",
    "\n",
    "# Composantes connexes\n",
    "print(\"\\nüîó Composantes connexes:\")\n",
    "components = list(nx.connected_components(G))\n",
    "print(f\"   Nombre: {len(components)}\")\n",
    "print(f\"   Plus grande: {len(max(components, key=len)):,} n≈ìuds ({len(max(components, key=len))/n_nodes*100:.1f}%)\")\n",
    "\n",
    "if len(components) > 1:\n",
    "    component_sizes = sorted([len(c) for c in components], reverse=True)\n",
    "    print(f\"   Top 5 tailles: {component_sizes[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e06cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des degr√©s\n",
    "print(\"\\nüìà Distribution des degr√©s:\")\n",
    "degrees = dict(G.degree())\n",
    "degree_values = list(degrees.values())\n",
    "\n",
    "print(f\"   Degr√© moyen: {np.mean(degree_values):.2f}\")\n",
    "print(f\"   Degr√© m√©dian: {np.median(degree_values):.0f}\")\n",
    "print(f\"   Degr√© min: {min(degree_values)}\")\n",
    "print(f\"   Degr√© max: {max(degree_values)}\")\n",
    "print(f\"   Std: {np.std(degree_values):.2f}\")\n",
    "\n",
    "# N≈ìuds les plus connect√©s\n",
    "top_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(f\"\\nüåü Top 5 n≈ìuds par degr√©:\")\n",
    "for node_id, degree in top_nodes:\n",
    "    title = G.nodes[node_id].get('title', 'N/A')[:60]\n",
    "    print(f\"   Node {node_id}: {degree} connexions - {title}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e8773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient\n",
    "print(\"\\nüî∫ Coefficient de clustering:\")\n",
    "avg_clustering = nx.average_clustering(G, weight='weight')\n",
    "print(f\"   Moyenne: {avg_clustering:.4f}\")\n",
    "\n",
    "if avg_clustering > 0.5:\n",
    "    print(\"   ‚úÖ Fort clustering (structure hi√©rarchique)\")\n",
    "elif avg_clustering > 0.3:\n",
    "    print(\"   ‚úÖ Clustering mod√©r√©\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Faible clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dd1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des poids des ar√™tes\n",
    "print(\"\\n‚öñÔ∏è  Distribution des poids des ar√™tes:\")\n",
    "edge_weights = [data['weight'] for _, _, data in G.edges(data=True)]\n",
    "\n",
    "print(f\"   Poids moyen: {np.mean(edge_weights):.4f}\")\n",
    "print(f\"   Poids m√©dian: {np.median(edge_weights):.4f}\")\n",
    "print(f\"   Poids min: {min(edge_weights):.4f}\")\n",
    "print(f\"   Poids max: {max(edge_weights):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cd4fa",
   "metadata": {},
   "source": [
    "## üíæ 5. Sauvegarde des R√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üíæ SAUVEGARDE DES R√âSULTATS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Sauvegarder le graphe NetworkX\n",
    "graph_path = OUTPUT_PATH / 'article_graph.gpickle'\n",
    "nx.write_gpickle(G, graph_path)\n",
    "print(f\"\\n‚úÖ Graphe NetworkX: {graph_path}\")\n",
    "print(f\"   Taille: {graph_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# 2. Sauvegarder les articles avec communaut√©s\n",
    "csv_path = OUTPUT_PATH / 'articles_with_communities.csv'\n",
    "articles_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Articles + communaut√©s: {csv_path}\")\n",
    "print(f\"   {len(articles_df):,} articles avec {n_communities} communaut√©s\")\n",
    "\n",
    "# 3. Sauvegarder les m√©tadonn√©es\n",
    "metadata = {\n",
    "    'graph_stats': {\n",
    "        'n_nodes': n_nodes,\n",
    "        'n_edges': n_edges,\n",
    "        'density': float(density),\n",
    "        'n_communities': n_communities,\n",
    "        'modularity': float(modularity),\n",
    "        'avg_clustering': float(avg_clustering),\n",
    "        'avg_degree': float(np.mean(degree_values))\n",
    "    },\n",
    "    'construction_params': {\n",
    "        'k_neighbors': K_NEIGHBORS,\n",
    "        'similarity_threshold': SIMILARITY_THRESHOLD,\n",
    "        'use_cosine': USE_COSINE,\n",
    "        'embedding_dim': embeddings.shape[1]\n",
    "    },\n",
    "    'community_stats': {\n",
    "        'n_communities': n_communities,\n",
    "        'avg_size': float(community_sizes.mean()),\n",
    "        'median_size': float(community_sizes.median()),\n",
    "        'max_size': int(community_sizes.max()),\n",
    "        'min_size': int(community_sizes.min())\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_PATH / 'graph_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"\\n‚úÖ M√©tadonn√©es: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc70533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Export pour PyTorch Geometric (GNN)\n",
    "print(\"\\nüî• Export pour PyTorch Geometric...\")\n",
    "\n",
    "# Cr√©er edge_index (format COO)\n",
    "edge_list = list(G.edges())\n",
    "edge_index = np.array([[e[0], e[1]] for e in edge_list] + \n",
    "                      [[e[1], e[0]] for e in edge_list])  # Bidirectionnel\n",
    "edge_index = edge_index.T  # Shape: (2, num_edges)\n",
    "\n",
    "# Cr√©er edge_attr (poids)\n",
    "edge_weights_list = [G[e[0]][e[1]]['weight'] for e in edge_list]\n",
    "edge_attr = np.array(edge_weights_list + edge_weights_list)  # Bidirectionnel\n",
    "\n",
    "# Sauvegarder\n",
    "pyg_data = {\n",
    "    'edge_index': edge_index,\n",
    "    'edge_attr': edge_attr,\n",
    "    'x': embeddings,  # Node features\n",
    "    'y': communities,  # Node labels (communaut√©s)\n",
    "    'num_nodes': n_nodes\n",
    "}\n",
    "\n",
    "pyg_path = OUTPUT_PATH / 'graph_pyg_format.pkl'\n",
    "with open(pyg_path, 'wb') as f:\n",
    "    pickle.dump(pyg_data, f)\n",
    "\n",
    "print(f\"‚úÖ Format PyG: {pyg_path}\")\n",
    "print(f\"   edge_index shape: {edge_index.shape}\")\n",
    "print(f\"   edge_attr shape: {edge_attr.shape}\")\n",
    "print(f\"   x shape: {embeddings.shape}\")\n",
    "print(f\"   y shape: {communities.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5051e",
   "metadata": {},
   "source": [
    "## üìä 6. Visualisations Professionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7542ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä VISUALISATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cr√©er la figure\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Distribution des degr√©s\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(degree_values, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax1.set_xlabel('Degr√©', fontsize=11)\n",
    "ax1.set_ylabel('Nombre de n≈ìuds', fontsize=11)\n",
    "ax1.set_title('Distribution des Degr√©s', fontsize=12, fontweight='bold')\n",
    "ax1.axvline(np.mean(degree_values), color='red', linestyle='--', \n",
    "            label=f'Moyenne: {np.mean(degree_values):.1f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution des poids d'ar√™tes\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(edge_weights, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax2.set_xlabel('Poids de similarit√©', fontsize=11)\n",
    "ax2.set_ylabel('Nombre d\\'ar√™tes', fontsize=11)\n",
    "ax2.set_title('Distribution des Poids d\\'Ar√™tes', fontsize=12, fontweight='bold')\n",
    "ax2.axvline(SIMILARITY_THRESHOLD, color='red', linestyle='--',\n",
    "            label=f'Seuil: {SIMILARITY_THRESHOLD}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Tailles des communaut√©s\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "top_comms = community_sizes.head(15)\n",
    "ax3.barh(range(len(top_comms)), top_comms.values, color='mediumseagreen', alpha=0.8)\n",
    "ax3.set_yticks(range(len(top_comms)))\n",
    "ax3.set_yticklabels([f'Comm {i}' for i in top_comms.index])\n",
    "ax3.set_xlabel('Nombre d\\'articles', fontsize=11)\n",
    "ax3.set_title('Top 15 Communaut√©s', fontsize=12, fontweight='bold')\n",
    "ax3.invert_yaxis()\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Distribution log des degr√©s\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.hist(degree_values, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "ax4.set_xlabel('Degr√©', fontsize=11)\n",
    "ax4.set_ylabel('Nombre de n≈ìuds (log)', fontsize=11)\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_title('Distribution des Degr√©s (√©chelle log)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Statistiques globales\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "STATISTIQUES DU GRAPHE\n",
    "{'='*40}\n",
    "\n",
    "üìä Structure:\n",
    "   ‚Ä¢ N≈ìuds: {n_nodes:,}\n",
    "   ‚Ä¢ Ar√™tes: {n_edges:,}\n",
    "   ‚Ä¢ Densit√©: {density:.6f}\n",
    "   ‚Ä¢ Degr√© moyen: {np.mean(degree_values):.2f}\n",
    "\n",
    "üß© Communaut√©s:\n",
    "   ‚Ä¢ Nombre: {n_communities}\n",
    "   ‚Ä¢ Taille moyenne: {community_sizes.mean():.1f}\n",
    "   ‚Ä¢ Modularit√©: {modularity:.4f}\n",
    "\n",
    "üî∫ Clustering:\n",
    "   ‚Ä¢ Coefficient: {avg_clustering:.4f}\n",
    "\n",
    "‚öñÔ∏è  Poids des ar√™tes:\n",
    "   ‚Ä¢ Moyenne: {np.mean(edge_weights):.4f}\n",
    "   ‚Ä¢ M√©diane: {np.median(edge_weights):.4f}\n",
    "\"\"\"\n",
    "ax5.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "         family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "# 6. Distribution cumulative des degr√©s\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "sorted_degrees = np.sort(degree_values)\n",
    "cumulative = np.arange(1, len(sorted_degrees) + 1) / len(sorted_degrees)\n",
    "ax6.plot(sorted_degrees, cumulative, linewidth=2, color='darkblue')\n",
    "ax6.set_xlabel('Degr√©', fontsize=11)\n",
    "ax6.set_ylabel('Proportion cumulative', fontsize=11)\n",
    "ax6.set_title('Distribution Cumulative des Degr√©s', fontsize=12, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "ax6.set_xlim(left=0)\n",
    "\n",
    "# 7. Heatmap des m√©triques par communaut√© (top 20)\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "top_20_comms = community_sizes.head(20).index\n",
    "comm_metrics = []\n",
    "for comm_id in top_20_comms:\n",
    "    nodes_in_comm = [n for n in G.nodes() if communities[n] == comm_id]\n",
    "    subgraph = G.subgraph(nodes_in_comm)\n",
    "    comm_metrics.append([\n",
    "        len(nodes_in_comm),\n",
    "        subgraph.number_of_edges(),\n",
    "        nx.density(subgraph) if len(nodes_in_comm) > 1 else 0,\n",
    "        np.mean([d for _, d in subgraph.degree()]) if len(nodes_in_comm) > 0 else 0\n",
    "    ])\n",
    "\n",
    "comm_metrics_df = pd.DataFrame(comm_metrics, \n",
    "                               columns=['Taille', 'Ar√™tes', 'Densit√©', 'Degr√© moy'],\n",
    "                               index=[f'C{i}' for i in top_20_comms])\n",
    "\n",
    "# Normaliser pour la heatmap\n",
    "comm_metrics_norm = (comm_metrics_df - comm_metrics_df.min()) / (comm_metrics_df.max() - comm_metrics_df.min())\n",
    "sns.heatmap(comm_metrics_norm.T, annot=comm_metrics_df.T, fmt='.0f', \n",
    "            cmap='YlOrRd', cbar_kws={'label': 'Valeur normalis√©e'},\n",
    "            linewidths=0.5, ax=ax7)\n",
    "ax7.set_title('M√©triques des Top 20 Communaut√©s', fontsize=12, fontweight='bold')\n",
    "ax7.set_xlabel('Communaut√©', fontsize=11)\n",
    "ax7.set_ylabel('M√©trique', fontsize=11)\n",
    "\n",
    "plt.suptitle('Analyse Compl√®te du Graphe d\\'Articles Scientifiques', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# Sauvegarder\n",
    "viz_path = OUTPUT_PATH / 'graph_analysis.png'\n",
    "plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Visualisation sauvegard√©e: {viz_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualisations termin√©es!\")\n",
    "\n",
    "# BONUS: Visualisation UMAP des embeddings par communaut√©\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    \n",
    "    print(\"\\nüó∫Ô∏è  G√©n√©ration de la visualisation UMAP...\")\n",
    "    \n",
    "    # √âchantillonner si trop grand\n",
    "    sample_size = min(3000, len(embeddings))\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.RandomState(42).choice(len(embeddings), sample_size, replace=False)\n",
    "        emb_sample = embeddings[indices]\n",
    "        comm_sample = communities[indices]\n",
    "    else:\n",
    "        emb_sample = embeddings\n",
    "        comm_sample = communities\n",
    "    \n",
    "    # UMAP\n",
    "    reducer = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    embedding_2d = reducer.fit_transform(emb_sample)\n",
    "    \n",
    "    # Visualiser\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "                         c=comm_sample, s=15, alpha=0.6, cmap='tab20')\n",
    "    plt.colorbar(scatter, label='ID Communaut√©')\n",
    "    plt.title(f'Projection UMAP des Embeddings (color√©s par communaut√©)\\n{sample_size} articles √©chantillonn√©s',\n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('UMAP 1', fontsize=12)\n",
    "    plt.ylabel('UMAP 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    umap_path = OUTPUT_PATH / 'embeddings_umap_communities.png'\n",
    "    plt.savefig(umap_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úÖ UMAP sauvegard√©: {umap_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  UMAP non install√© (pip install umap-learn)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ CONSTRUCTION DU GRAPHE TERMIN√âE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Fichiers g√©n√©r√©s dans {OUTPUT_PATH}:\")\n",
    "print(f\"   1. article_graph.gpickle - Graphe NetworkX complet\")\n",
    "print(f\"   2. articles_with_communities.csv - Articles + communaut√©s\")\n",
    "print(f\"   3. graph_metadata.json - M√©tadonn√©es et statistiques\")\n",
    "print(f\"   4. graph_pyg_format.pkl - Format PyTorch Geometric\")\n",
    "print(f\"   5. graph_analysis.png - Visualisations compl√®tes\")\n",
    "print(f\"   6. embeddings_umap_communities.png - Projection UMAP\")\n",
    "\n",
    "print(f\"\\nüìä R√©sum√© final:\")\n",
    "print(f\"   ‚Ä¢ {n_nodes:,} articles dans le graphe\")\n",
    "print(f\"   ‚Ä¢ {n_edges:,} connexions s√©mantiques\")\n",
    "print(f\"   ‚Ä¢ {n_communities} communaut√©s d√©tect√©es\")\n",
    "print(f\"   ‚Ä¢ Modularit√©: {modularity:.4f}\")\n",
    "print(f\"   ‚Ä¢ Pr√™t pour GNN et analyses avanc√©es!\")\n",
    "\n",
    "\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üî• 7. Exemple d'Utilisation avec PyTorch Geometric\\n\",\n",
    "    \"\\n\",\n",
    "    \"Code pour charger et utiliser le graphe avec des GNN\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Exemple de chargement pour PyTorch Geometric\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n\",\n",
    "    \"print(\\\"üî• EXEMPLE PYTORCH GEOMETRIC\\\")\\n\",\n",
    "    \"print(\\\"=\\\"*70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Code d'exemple (n√©cessite: pip install torch-geometric)\\n\",\n",
    "    \"example_code = \\\"\\\"\\\"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from torch_geometric.data import Data\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Charger les donn√©es\\n\",\n",
    "    \"with open('graph_outputs/graph_pyg_format.pkl', 'rb') as f:\\n\",\n",
    "    \"    pyg_data = pickle.load(f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Cr√©er l'objet Data PyG\\n\",\n",
    "    \"data = Data(\\n\",\n",
    "    \"    x=torch.FloatTensor(pyg_data['x']),           # Node features (embeddings)\\n\",\n",
    "    \"    edge_index=torch.LongTensor(pyg_data['edge_index']),  # Edges\\n\",\n",
    "    \"    edge_attr=torch.FloatTensor(pyg_data['edge_attr']).unsqueeze(1),  # Edge weights\\n\",\n",
    "    \"    y=torch.LongTensor(pyg_data['y'])             # Labels (communities)\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Graph loaded: {data}\\\")\\n\",\n",
    "    \"print(f\\\"  - Nodes: {data.num_nodes}\\\")\\n\",\n",
    "    \"print(f\\\"  - Edges: {data.num_edges}\\\")\\n\",\n",
    "    \"print(f\\\"  - Features: {data.num_node_features}\\\")\\n\",\n",
    "    \"print(f\\\"  - Classes: {data.y.unique().numel()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Exemple de GNN simple\\n\",\n",
    "    \"from torch_geometric.nn import GCNConv, global_mean_pool\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"\\n\",\n",
    "    \"class ArticleGNN(torch.nn.Module):\\n\",\n",
    "    \"    def __init__(self, in_channels, hidden_channels, num_classes):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.conv1 = GCNConv(in_channels, hidden_channels)\\n\",\n",
    "    \"        self.conv2 = GCNConv(hidden_channels, hidden_channels)\\n\",\n",
    "    \"        self.conv3 = GCNConv(hidden_channels, num_classes)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def forward(self, x, edge_index, edge_weight=None):\\n\",\n",
    "    \"        x = self.conv1(x, edge_index, edge_weight)\\n\",\n",
    "    \"        x = F.relu(x)\\n\",\n",
    "    \"        x = F.dropout(x, p=0.5, training=self.training)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        x = self.conv2(x, edge_index, edge_weight)\\n\",\n",
    "    \"        x = F.relu(x)\\n\",\n",
    "    \"        x = F.dropout(x, p=0.5, training=self.training)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        x = self.conv3(x, edge_index, edge_weight)\\n\",\n",
    "    \"        return F.log_softmax(x, dim=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 4. Initialiser le mod√®le\\n\",\n",
    "    \"model = ArticleGNN(\\n\",\n",
    "    \"    in_channels=data.num_node_features,\\n\",\n",
    "    \"    hidden_channels=64,\\n\",\n",
    "    \"    num_classes=data.y.unique().numel()\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nMod√®le cr√©√©: {model}\\\")\\n\",\n",
    "    \"print(f\\\"Nombre de param√®tres: {sum(p.numel() for p in model.parameters())}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 5. Training loop (exemple simplifi√©)\\n\",\n",
    "    \"# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\\n\",\n",
    "    \"# criterion = torch.nn.NLLLoss()\\n\",\n",
    "    \"# ...\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüìù Code d'exemple pour GNN:\\\")\\n\",\n",
    "    \"print(example_code)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüí° Pour utiliser ce code:\\\")\\n\",\n",
    "    \"print(\\\"   1. pip install torch torch-geometric\\\")\\n\",\n",
    "    \"print(\\\"   2. Chargez graph_pyg_format.pkl\\\")\\n\",\n",
    "    \"print(\\\"   3. Adaptez l'architecture GNN √† vos besoins\\\")\\n\",\n",
    "    \"print(\\\"   4. Entra√Ænez sur les communaut√©s ou d'autres t√¢ches\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
