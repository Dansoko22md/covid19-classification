{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction du graphe d'articles et communaut√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from community import community_louvain\n",
    "import matplotlib.pyplot as plt\n",
    "BASE_PATH = Path(\"../data/processed\")\n",
    "EMB_PATH = BASE_PATH / \"embeddings.npy\"\n",
    "DF1 = BASE_PATH / \"articles_for_embeddings.csv\"\n",
    "DF2 = BASE_PATH / \"cleaned_articles.csv\"\n",
    "K = 10\n",
    "SIM_THRESHOLD = 0.2\n",
    "USE_COSINE = True\n",
    "SAMPLE_SIZE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Chargement des embeddings et des articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lapte\\AppData\\Local\\Temp\\ipykernel_2268\\2048823343.py:6: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DF1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 772,650 articles, dim=384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Clinical features of culture-proven Mycoplasma...\n",
       "1  Nitric oxide: a pro-inflammatory mediator in l..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üìÇ Chargement des embeddings et des articles...\")\n",
    "if not EMB_PATH.exists():\n",
    "    raise FileNotFoundError(str(EMB_PATH))\n",
    "emb = np.load(EMB_PATH)\n",
    "if DF1.exists():\n",
    "    df = pd.read_csv(DF1)\n",
    "elif DF2.exists():\n",
    "    df = pd.read_csv(DF2)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Aucun CSV d'articles trouv√©\")\n",
    "n = min(len(df), emb.shape[0])\n",
    "df = df.iloc[:n].reset_index(drop=True)\n",
    "emb = emb[:n]\n",
    "if SAMPLE_SIZE and n > SAMPLE_SIZE:\n",
    "    df = df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    emb = emb[df.index.values]\n",
    "print(f\"‚úÖ {len(df):,} articles, dim={emb.shape[1]}\")\n",
    "df[['title']].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Construction k-NN...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     nn = NearestNeighbors(n_neighbors=K+\u001b[32m1\u001b[39m, metric=\u001b[33m\"\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     nn.fit(emb_n)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     dists, indices = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     sims = \u001b[32m1.0\u001b[39m - dists\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:906\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m         kwds = \u001b[38;5;28mself\u001b[39m.effective_metric_params_\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     chunked_results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mball_tree\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkd_tree\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2249\u001b[39m, in \u001b[36mpairwise_distances_chunked\u001b[39m\u001b[34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[39m\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2248\u001b[39m     chunk_size = D_chunk.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2249\u001b[39m     D_chunk = \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:750\u001b[39m, in \u001b[36mKNeighborsMixin._kneighbors_reduce_func\u001b[39m\u001b[34m(self, dist, start, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    723\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reduce a chunk of distances to the nearest neighbors.\u001b[39;00m\n\u001b[32m    724\u001b[39m \n\u001b[32m    725\u001b[39m \u001b[33;03mCallback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    747\u001b[39m \u001b[33;03m    The neighbors indices.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    749\u001b[39m sample_range = np.arange(dist.shape[\u001b[32m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m neigh_ind = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m neigh_ind = neigh_ind[:, :n_neighbors]\n\u001b[32m    752\u001b[39m \u001b[38;5;66;03m# argpartition doesn't guarantee sorted order, so we sort again\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:961\u001b[39m, in \u001b[36margpartition\u001b[39m\u001b[34m(a, kth, axis, kind, order)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[32m    876\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margpartition\u001b[39m(a, kth, axis=-\u001b[32m1\u001b[39m, kind=\u001b[33m'\u001b[39m\u001b[33mintroselect\u001b[39m\u001b[33m'\u001b[39m, order=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    877\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    878\u001b[39m \u001b[33;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[33;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m \n\u001b[32m    960\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margpartition\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lapte\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"üîó Construction k-NN...\")\n",
    "if USE_COSINE:\n",
    "    norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1.0\n",
    "    emb_n = emb / norms\n",
    "    nn = NearestNeighbors(n_neighbors=K+1, metric=\"cosine\")\n",
    "    nn.fit(emb_n)\n",
    "    dists, indices = nn.kneighbors(emb_n, return_distance=True)\n",
    "    sims = 1.0 - dists\n",
    "else:\n",
    "    nn = NearestNeighbors(n_neighbors=K+1, metric=\"euclidean\")\n",
    "    nn.fit(emb)\n",
    "    dists, indices = nn.kneighbors(emb, return_distance=True)\n",
    "    sims = 1.0 / (1.0 + dists)\n",
    "edges = []\n",
    "for i in range(indices.shape[0]):\n",
    "    for j in range(1, indices.shape[1]):\n",
    "        nbr = int(indices[i, j])\n",
    "        w = float(sims[i, j])\n",
    "        if w >= SIM_THRESHOLD:\n",
    "            edges.append((i, nbr, w))\n",
    "len(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß± Cr√©ation du graphe NetworkX...\")\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(df)))\n",
    "for i, row in df.iterrows():\n",
    "    G.nodes[i]['title'] = row.get('title','')\n",
    "for i, j, w in edges:\n",
    "    if i != j:\n",
    "        if G.has_edge(i, j):\n",
    "            if w > G[i][j].get('weight', 0.0):\n",
    "                G[i][j]['weight'] = w\n",
    "        else:\n",
    "            G.add_edge(i, j, weight=w)\n",
    "G.number_of_nodes(), G.number_of_edges()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß© D√©tection des communaut√©s (Louvain)...\")\n",
    "partition = community_louvain.best_partition(G, weight='weight', random_state=42)\n",
    "comm = pd.Series(partition).sort_index().values\n",
    "df_out = df.copy()\n",
    "df_out['community'] = comm\n",
    "df_out.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = BASE_PATH / 'article_graph.gpickle'\n",
    "csv_path = BASE_PATH / 'articles_with_communities.csv'\n",
    "nx.write_gpickle(G, graph_path)\n",
    "df_out.to_csv(csv_path, index=False)\n",
    "print(f\"üíæ Graphe: {graph_path}\")\n",
    "print(f\"üíæ Communaut√©s: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Statistiques\")\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges()\n",
    "deg = [d for _, d in G.degree()]\n",
    "n_comm = int(df_out['community'].nunique())\n",
    "sizes = df_out['community'].value_counts().head(10)\n",
    "print(f\"Noeuds: {n_nodes:,}, Ar√™tes: {n_edges:,}, Communaut√©s: {n_comm}\")\n",
    "print(\"Top tailles de communaut√©s:\")\n",
    "print(sizes.to_string())\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(deg, bins=50, edgecolor='black')\n",
    "plt.title('Distribution des degr√©s')\n",
    "plt.xlabel('Degr√©')\n",
    "plt.ylabel(\"Nombre d'articles\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "print(\"üó∫Ô∏è Visualisation UMAP par communaut√© (√©chantillon)\")\n",
    "n = min(2000, emb.shape[0])\n",
    "ix = np.random.RandomState(42).choice(emb.shape[0], n, replace=False)\n",
    "emb_s = emb[ix]\n",
    "lab_s = df_out.iloc[ix]['community'].values\n",
    "um = UMAP(n_components=2, random_state=42)\n",
    "xy = um.fit_transform(emb_s)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(xy[:,0], xy[:,1], c=lab_s, s=4, cmap='tab20', alpha=0.7)\n",
    "plt.title('UMAP des embeddings color√©s par communaut√©')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (PySpark)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
